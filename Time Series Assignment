---
title: "MATH1318 Time Series Analysis"
subtitle: "Time Series Analysis Final Project Report"
date: "2022-06-12"
output:
  html_document:
    df_print: paged
    toc: true
    toc_float: true
  pdf_document: default
  html_notebook: default
---



```{r, message=FALSE, warning=FALSE,message=FALSE }
# Load the necessary packages
library(magrittr)
library(kableExtra)
library(TSA)
library(tidyverse)
library(tseries)
library(lmtest)
library(fUnitRoots)
library(forecast)

```


## **1. Introduction**
This report represents a study to understand the characteristics of rainfall level in Melbourne Airport area. The dataset used in this report is composed of observations from the **annual precipitation data at Melbourne airport from 1971 to 2021**, published by *the Bureau of Meteorology*. The study objective is to find the most accurate forecasting methods for predicting the annual rainfall level at this location for the next 10 years. 




## **2. Data Pre-processing and Descriptive Analysis**

### **2.1 Dataset summary**

In the initial data processing, we have extracted the annual rainfall data. Now the data has two variables including year and total annual rainfall per year in millimetres. It consists of total 51 observations from 1971 to 2021, with an average of rainfall level of 537.5mm per year. The data poits are complete and without missing values.



```{r, message=FALSE, warning=FALSE}
#read data, select annual rainfall data and valid entries
data <-read.csv('IDCJAC0001_086282_Data12.csv',header =T)%>%.[2:(nrow(.)-1),c(3,16)]
#convert to numeric type if it is of char type
data<-data%>%mutate_if(is.character,as.numeric)
#check the first five data points
head(data)
#number of observations
print(paste0("Total Observations: ",nrow(data)))
#summary of data
summary(data)

```




### **2.2 Descriptive Analysis**

The data was then converted into a time series object. We have also created  **time series plot, scatter plot and ACF and PACF plots** for understanding the overall patterns in the times series data.


From the plots we've created, we can say that the dataset represents the follwoing major characteristics:


  
  * **Trend**: There is no consistent upward or downward trend in Fig.1 at the broader time frame in the original time series data.
  
  * **Seasonality**: There is no distinctive repetitive patterns or cycles showed in Fig.1.
  
  * **Changing Variance**: There are some changes in variance. We can observe some fluctuations in consecutive data points but are not obvious in broader time frame.
  
  * **Change point**: No distictive change point observed in the time series data neither.
  
  * **Behavior**: The time series exhibits some moving average patterns but has no obvious autoregressive behaviors based on the time series plot (Fig.1). Also When look into the scatter plot, we observed low **correlation**  between succeeding data points (Fig.2). And the calculated correlation value is only 0.029. Figure 3.a and Figure 3.b (ACF and PACF plots) also show that all bars are within the limit and no distinctive wave patterns observed. 
  
  
  

**Convert to time series**

```{r, message=FALSE, warning=FALSE}
#convert to time series
ts_data<-ts(data$Annual,start =1971,end=2021, frequency = 1)

```



**Time Series Plot**
```{r, message=FALSE, warning=FALSE, fig.align="center"}
#adjust margin of the plots
par(mar=c(5,4,4,2),cex.main =1,cex.lab=1,cex.axis=1)

#plot the time seires
plot(ts_data,ylab='millimetres',xlab='Year',type='o', 
  main = "Fig.1 Annual Total Precipitation at Melbourne Airport from 1971")

```
**Scatter Plot**
```{r, message=FALSE, warning=FALSE, fig.align="center"}
#adjust margin of the plots
par(mar=c(5,4,4,2),cex.main =1,cex.lab=1,cex.axis=1)

y <- ts_data
x <- zlag(ts_data)#first lag
index <- 2:length(x) # Create an index to get rid of the first NA value in x
plot(y[index],x[index],ylab='millimetres', xlab='The first lag', main = 'Fig.2: Scatter Plot of TS and its first lag')

# Correlation
cor(y[index],x[index])


```


**ACF/PACF Plot**
```{r, message=FALSE, warning=FALSE, fig.align="center"}
#adjust margin of the plots
par(mar=c(5,4,4,2),cex.main =1,cex.lab=1,cex.axis=1)
#Acf plot
acfplot<-acf(ts_data, main = "Figure 3.a ACF of the time series.")
#PACF
pacfplot<-pacf(ts_data, main = "Figure 3.b PACF of the time series.")

```



## **3. Model Specifications**




### **3.1 Check Stationarity & Normality**


Now that we have looked at the descriptive statistics for our model, it is time for us determine what ARIMA models would be suitable for our model. An important step when working with ARIMA models is to ensure that our data is **stationary**. Stationary data is when the mean and variance of the time series is stationary over time, compared to no-stationary data where the mean and variance change throughout the time series. A constant mean and variance in the data makes the time series much easier to predict and more suitable for forecasting, which is why we need to ensure we use a stationary model for our data.



```{r, message=FALSE, warning=FALSE}

#ADF test to test for stationarity. p-value of 0.098 is threshold value.
adf.test(ts_data)

#Phillips–Perron test to test for stationarity. P-value less than 0.05 indicates series is stationary
pp.test(ts_data, alternative = c("stationary"))


```


Looking back at  Fig.1 we can see that the original time series for our data appears to be mostly stationary, with no apparent change in mean or variance throughout the time series. The ADF and PP tests were conducted for checking the stationary on our original data set.  The results for the ADF test gives us a p-value that is greater than our standard alpha of 0.05. This means that from the result of the ADF test we can't conclude that our data is stationary from the results of the ADF tests. Although from the results to the PP test we can accept that the data is stationary since the p-value is less than 0.05, meaning we can reject the null hypothesis. However in order to try and make sure that our data is stationary, we are going to try a differencing model to see if we can get better results.

```{r, fig.align="center"}
#Shapiro test for normality
shapiro.test(ts_data)

#QQplot
qqnorm(ts_data,main ='Fig.4.QQ plot')
qqline(ts_data, col =2)

```

The shapiro test with a **p-value>0.05 **, indicating that the data is normal. When we check the qq plot, we can see that the data points are aligned relatively well. Thus transformation may not be necessary.


### **3.2 Differencing**

A differencing model looks at the difference between consecutive variables, this sort of transformation can help flatten out any trend in the data and turn a non-stationary time series into a stationary one. Figure 4 shows the time series plot for our differencing model, and of note is that our differencing model looks to have a more stable mean compared to our original time series plot back in figure 1. Just like in our original time series there also appears to be no changing variance, which again suggest that the model is stationary.
 

```{r, message=FALSE, warning=FALSE, fig.align="center"}

#differencing
rain.diff<-diff(ts_data)


#adjust margin of the plots
par(mar=c(5,4,4,2),cex.main =1,cex.lab=1,cex.axis=1)

#time seires plot for differenced data
plot(rain.diff, main = "Fig.5 Differencing Time Series for Rainfall", ylab = "Rainfall",
     xlab = "Time",type='o')
```

Now we can look at the results of the ADF and PP tests for our differencing model in figure 5. Both the ADF and PP tests for stationary give us p-values that are less than our standard alpha of 0.05, this means that we can reject the null hypothesis for both tests and conclude that our differencing model is stationary. Because of these improved results to the ADF and PP tests for our differencing model compared to our original data, this is the model that we are going to use to find suitable ARIMA models for forecasting.
 
```{r, message=FALSE, warning=FALSE}
#adf test of differencing data
adf.test(rain.diff)
#pp test of differencing data
pp.test(rain.diff,alternative = c("stationary"))
```




### **3.3 ACF/PACF Plot**

```{r, fig.align="center"}
#ACF and PACF plots of the first differencing data
par(mar=c(5,4,4,2),cex.main =1,cex.lab=1,cex.axis=1)
acf(rain.diff, main = "Fig.6a ACF for Rainfall Differencing")
pacf(rain.diff, main = "Fig.6b PACF for Rainfall Differencing")
```

Let's now have a look at finding the set of possible ARIMA models that we can use to forecast our data. Firstly we are going to look at Fig.6a and Fig.6b which showcases the ACF and PACF plots for our model. In the ACF plot we can observe that we only have one ACF value that passes our significant line, whilst in the PACF plot we can observe three PACF values that have passes our significance threshold. However one of these PACF values is only marginally above our threshold, meaning we should consider the possibility of using a model without that PACF value being significant. This means that possible ARIMA models from our ACF and PACF plots are:


  * ARIMA(3,1,1)
  
  * ARIMA(2,1,1)
  


### **3.4 EACF Test**
```{r, fig.align="center"}
#EACF
eacf(rain.diff,ar.max = 5,ma.max = 5)
```
Based on the EACF plot for our differencing model, from this EACF plot we can observe that the set of possible ARIMA models that it recommends are:

  * ARIMA(0,1,1)
  
  * ARIMA(2,1,1)
  
  * ARIMA(3,1,1)
  
  * ARIMA(0,1,2)
  
  * ARIMA(1,1,2)





### **3.5 BIC Table**

```{r, message=FALSE, warning=FALSE, fig.align="center"}
#BIC Table
res = armasubsets(y=rain.diff,nar=5,nma=5,y.name='test',ar.method='ols')
```

```{r, message=FALSE, warning=FALSE, fig.align="center"}
par(mar=c(5,4,4,2),cex.main =1,cex.lab=1,cex.axis=1)
plot(res)
```

The last plot we are going to look at too determine a set of possible models is the BIC plot. This plot can be seen above and the possible ARIMA models that it recommends are:

  * ARIMA(0,1,1)
  
  * ARIMA(2,1,1)
   
Through these plots we have been able to build a list of possible ARIMA models for our time series, that can be used to forecast future values. The list of possible models recommended to us by these plots are:

  * ARIMA(0,1,1)
  
  * ARIMA(2,1,1)
  
  * ARIMA(3,1,1)
  
  * ARIMA(0,1,2)
 
Now that we have transformed the model to be stationary and determined a possible set of ARIMA models for forecasting we will now move on to testing these possible models to figure out which model will be the best for forecasting.




## **4. Parameter Estimation**

After model specification, parameter estimation tests were conducted on the following models to narrow down the best model for selection: 

  * ARIMA(0,1,1)
  
  * ARIMA(2,1,1)
  
  * ARIMA(3,1,1)
  
  * ARIMA(0,1,2)
  
  * ARIMA(1,1,2)


### **4.1 Model Fitting**

Least ordinary squares method was first used from the ‘stats’ and ‘TSA’ package on  the ARIMA(0,1,1) model and returned significant coefficient p-values less than 0.001 for first order moving average(MA1).

From the output for each model for parameter estimation below, we can see all models appear to be significant and possible except for ARIMA(1,1,2), where the MA2 value is greater than alpha value of 0.05, thus this model was discounted.

**ARIMA(0,1,1)**
```{r}
# ARIMA(0,1,1)
model_011_CSS <- arima(rain.diff, order = c(0,1,1), method = "CSS")
coeftest(model_011_CSS) # p-value < 0.05. Est Std. = -0.917584

model_011 <- arima(rain.diff, order = c(0,1,1), method = "ML")
coeftest(model_011) # p-value < 0.05 Est. Std. = -0.999986

model_011_ML_CSS <- arima(rain.diff, order = c(0,1,1), method = "CSS-ML")
coeftest(model_011_ML_CSS) # p-value < 0.05 Est. Std. = -.0999972
```


**ARIMA(2,1,1)**
```{r}
# ARIMA(2,1,1)
model_211_CSS <- arima(rain.diff, order = c(2,1,1), method = "CSS")
coeftest(model_211_CSS) # p-value < 0.05. 

model_211 <- arima(rain.diff, order = c(2,1,1), method = "ML")
coeftest(model_211) # p-value < 0.05. 

model_211_CSS_ML <- arima(rain.diff, order = c(2,1,1), method = "CSS-ML")
coeftest(model_211_CSS_ML) # p-value < 0.05. 
```

**ARIMA(3,1,1)**
```{r}
# ARIMA(3,1,1)
model_311_CSS <- arima(rain.diff, order = c(3,1,1), method = "CSS")
coeftest(model_311_CSS) # p-value < 0.05. 

model_311 <- arima(rain.diff, order = c(3,1,1), method = "ML")
coeftest(model_311) # p-value < 0.05. 

model_311_CSS_ML <- arima(rain.diff, order = c(3,1,1), method = "CSS-ML")
coeftest(model_311_CSS_ML) # p-value < 0.05.
```

**ARIMA(0,1,2)**
```{r}
# ARIMA(0,1,2)
model_012 <- arima(rain.diff, order = c(0,1,2), method = "ML")
coeftest(model_012) # p-value < 0.05. 

model_012_CSS <- arima(rain.diff, order = c(0,1,2), method = "CSS")
coeftest(model_012_CSS) # p-value < 0.05.

model_012_CSS_ML <- arima(rain.diff, order = c(0,1,2), method = "CSS-ML")
coeftest(model_012_CSS_ML) # p-value < 0.05.
```

**ARIMA(1,1,2)**
```{r}
# ARIMA(1,1,2) 
model_112 <- arima(rain.diff, order = c(1,1,2), method = "ML")
coeftest(model_112) # p-value > 0.05 NOT GOOD

model_112_CSS <- arima(rain.diff, order = c(1,1,2), method = "CSS")
coeftest(model_112_CSS) # p-value > 0.05 NOT GOOD

model_112_CSS_ML <- arima(rain.diff, order = c(1,1,2), method = "CSS-ML")
coeftest(model_112_CSS_ML) # p-value > 0.05 NOT GOOD
```
### **4.2 AIC/BIC**

An Akaike and Bayesian Information Criterion (AIC and BIC) was produced for model selection of the remaining ARIMA models. From the two tables it was concluded that the model with the lowest prediction error was **ARIMA(0,1,2)**; an indicator of a good possible model. 
 


```{r}
#Custom function for BIC and AIC tables
sort.score <- function(x, score = c("bic", "aic")){
  if (score == "aic") {
    x[with(x, order(AIC)),]
  } else if (score == "bic") {
    x[with(x, order(BIC)),]
  } else {
    warning('score = "x" only accepts valid arguments ("aic", "bic")')
  }
}
```

**AIC tables sorted**
```{r}
# smallest AIC is model ARIMA(0,1,2) 
sort.score(AIC(model_011,model_012, model_112, model_211, model_311), score = "aic")
```
**BIC tables sorted**
```{r}
# smallest BIC is model ARIMA(0,1,2)
sort.score(BIC(model_011,model_012, model_112, model_211, model_311), score = "bic")
```
Following this, a summary of residual errors was tabulated to determine the accuracy of each model. 
From the table we can see that ARIMA(1,1,2) and ARIMA(0,1,2) have low and similar RMSE, MAE and MASE scores, however, because the ARIMA(0,1,2) model is also supported by the low AIC and BIC error scores we have chosen this as our model of selection.                                                                                       
The next step was to perform residual diagnostic on ARIMA(0,1,2). 

```{r}
#Accuracy summary
model_012A = Arima(rain.diff, order = c(0,1,2), method='ML')
model_112A = Arima(rain.diff, order = c(1,1,2), method='ML')
model_311A = Arima(rain.diff, order = c(3,1,1), method='ML')
model_211A = Arima(rain.diff, order = c(2,1,1), method='ML')
model_011A = Arima(rain.diff, order = c(0,1,1), method='ML')

Smodel_012A <- accuracy(model_012A)#[1:7]
Smodel_112A <- accuracy(model_112A)#[1:7] 
Smodel_311A <- accuracy(model_311A)#[1:7]
Smodel_211A <- accuracy(model_211A)#[1:7]
Smodel_011A <- accuracy(model_011A)#[1:7]

df.Smodels <- data.frame(
  rbind(Smodel_011A, Smodel_012A, Smodel_112A, Smodel_211A, Smodel_311A)
)
colnames(df.Smodels) <- c("ME", "RMSE", "MAE", "MPE", "MAPE", "MASE", "ACF1")
rownames(df.Smodels) <- c("ARIMA(0,1,1)", "ARIMA(0,1,2)", "ARIMA(1,1,2)", "ARIMA(2,1,1)", "ARIMA(3,1,1)")

cols<-colnames(df.Smodels)
df.Smodels[cols]<-lapply(df.Smodels,round,3)
df.Smodels
```


## **5. Residual Diagnostics**

Having found our best model, ARIMA(0,1,2) we will use overfitting to further detect any anomalies in the goodness of fit of our model. To do this, we will create two more models by separately increasing the orders of p and q by 1. So, our two overfitted models will be ARIMA(1,1,2) and ARIMA(0,1,3). We will check coefficients of these two models.

Looking at the ARIMA(1,1,2) model with an increased p order, we can see that the additional AR(1) parameter is insignificant with a p-value of 0.665. Additionally, our original MA(2) parameter has now become insignificant with a p-value of 0.132, whereas it is significant in the original ARIMA(0,1,2) model.
```{r}
coeftest(arima(rain.diff, order = c(1,1,2), method = "ML")) # additional AR(1) parameter is insignificant
```

Next, we look at the ARIMA(0,1,3) model with an increased order of q. The additional MA(3) parameter is insignificant at a p-value of 0.615. Additionally, our original MA(2) parameter has now become insignificant with a p-value of 0.067, whereas it is significant in the original ARIMA(0,1,2) model.

These two results indicate that our chosen ARIMA(0,1,2) model is indeed a suitable choice.
```{r}
coeftest(arima(rain.diff, order = c(0,1,3), method = "ML")) # additional AR(1) parameter is insignificant
```

Now we move on to the residual diagnostics of our ARIMA(0,1,2) model. Analysis is done via time series plot, histogram, QQ plot, ACF plot, PACF plot, Shapiro-Wilk test, and Box-Ljung test.

The time series plot of the standardised residuals shows a fairly random distribution of observations around the horizontal 0 value. It does not have any trend, seasonality, change point, variance, or behaviour.

The histogram is almost symmetrical, with nearly all values between -2 and 2 and no outliers. 

The QQ plot looks at normality of the residuals. We are expecting to see all data points following the red trend line. Our plot very nearly resembles this, with only the end range quantiles less than -1 and greater than 1 slightly veering away from the trend line.  

The Shapiro-Wilk test is also used to analyse normality. The test gives a large p-value of 0.500 which is well above the significance level of 0.05. With this result, we do not reject the null hypothesis and can say that the data is normal.

The ACF plot of residuals shows that all lags are within the blue dashed lines of the 95% confidence interval. This is what we expect to see when we have no significant autocorrelation between lags in the residuals.

In the PACF plot of residuals, we expect to see similar results to the ACF plot. That is, all lags are insignificant and lie with the 95% confidence intervals. That is exactly what our PACF plot shows.


```{r, fig.align="center"}
model_012Res = rstandard(model_012)
par(mar=c(5,4,4,2),cex.main =1,cex.lab=1,cex.axis=1)
#time series plot of modelresidual
plot(model_012Res, xlab = "Year", ylab = "Standardized Residuals", main = "Fig.7 Time Series Plot of Standardised Residuals", type = "o")
#residual histogram
hist(model_012Res, ylab = "Standardised Residuals", main = "Fig.8 Histogram of Standardised Residuals for Annual Rainfall")
qqnorm(model_012Res, main = "Fig.9 QQ plot of Standardised Residuals for Annual Rainfall")
qqline(model_012Res, col =2, lwd = 1, lty = 2)
#shapiro test
shapiro.test(model_012Res) # p-value 0.5 which indicates NORMALITY
#ACF/PACF of residuals
acf(model_012Res, main = "Fig.10a ACF plot of Standardised Residuals for Annual Rainfall")
pacf(model_012Res, main = "Fig.10b PACF plot of Standardised Residuals for Annual Rainfall")
```

Lastly, we look at a Box-Ljung test to find any significant autocorrelation in the residuals. The test gives us a p-value of 0.815, indicating no significant autocorrelation. The bottom plot of the below diagram shows us the Ljung-Box statistic p-values at many different lags. All the p-values are high, well above the 0.05 significance levels, which is what we are looking for in the graph. This indicates autocorrelation is insignificant at every lag for our model.
```{r, fig.align="center"}

Box.test(model_012Res, type = "Ljung-Box")
par(mar=c(5,4,4,2),cex.main =1,cex.lab=1,cex.axis=1)
tsdiag(model_012, gof =15, omit.initial=F)
```


## **6. Forecasting**

The last step of our project involves making predictions for the annual rainfall of the next 10 years. This is a very important step as it will be answering our initial research question. All of the previous analysis was completed in order to create the most suitable model to use for our forecasting. For the forecasting, we use the Arima and forecast functions on our chosen ARIMA(0,1,2) model. The 95% confidence interval gives a very large range for the annual rainfall of the next 10 years as there was not much of a trend in the original data set. The point table forecast also gives the same result of 531 mm for 9 out of 10 of the coming years due to the lack of trend, making it hard for the forecast model to predict how the rainfall will vary.


```{r, fig.align="center"}
model_012A <- Arima(ts_data, order=c(0,1,2), method = "ML")
model_012Afrc <- forecast::forecast(model_012A, h = 10)
plot(model_012Afrc, ylab = "Rainfall in mm", xlab = "Year", type='o',main = "Fig.11 Annual Rainfall Forecast For Next 10 Years")
```

## **7. Conclusion**


This project investigated the annual rainfall at Melbourne Airport from 1971 to 2021. Our research question was, what will be the annual rainfall at Melbourne Airport for the next 10 years? To find the answer to this, we had to create a stationary time series and find a suitable model to depict the behaviour of the time series. 


Initial descriptive analysis and visualisation showed our data set was possibly stationary. To confirm stationarity, we performed differencing on the original data set. In the model specification phase, we used ACF, PACF, and EACF plots, as well as a BIC table to come up with the following set of possible ARIMA models: ARIMA(0,1,1), ARIMA(2,1,1), ARIMA(3,1,1), ARIMA(0,1,2), and ARIMA(1,1,2). Then, we fit our models to find the best one by testing coefficients and creating AIC and BIC tables. We found that ARIMA(0,1,2) was the model with best fit. Residual diagnostics, including overfitting, ACF plots, PACF plots, histograms, QQ plots, and Ljung-Box diagrams were used to confirm that our chosen **ARIMA(0,1,2)** model was a good fit. 

 

Finally, we were able to use the ARIMA(0,1,2) model to answer our research question and predict the annual rainfall at Melbourne Airport for the next 10 years, which appears to be around the 531 millimetres per year mark. 



<br>
<br>
